Week_3 (0324-0326)
=============

**목차**
1. [워드 임베딩 Word Embedding](워드-임베딩-Word-Embedding)
2. [분포 의미론 Distributed Semantics](분포-의미론-Distributed-Semantics)
3. [Word2Vec](Word2Vec)

워드 임베딩 Word Embedding
=============
저번 주에는 NLP에서 어휘 속의 단어나 구(phrase)가 실수의 벡터로 매핑되는 언어 모델과 피처 엔지니어링 기술을 살펴보았다. 워드 임베딩이란, 단어를 실수로 표현하는데 사용하는 기술이다.

원-핫 인코딩으로 만든 벡터는 희소(sparse)하고 고차원이다. 이게 무슨 뜻이냐하면, 벡터값 대부분이 0이고(원-핫 인코딩을 하면 어떤 행렬이 나오는지 떠올려보자), 어휘 사전에 있는 단어의 수와 차원이 같다. 반면, 워드 인베딩은 저차원의 실수형 벡터(밀집 벡터)이다. 

<p align="center"><img width="35%" src="https://user-images.githubusercontent.com/80621384/112938737-24cbea00-9165-11eb-8a1b-584686579515.png" /></p>

워드 임베딩 방법론으로는 LSA, Word2Vec, FastText, Glove 등이 있다. 그중 Word2Vec을 중점적으로 살펴보려고 한다.

분포 의미론 Distributed Semantics
=============
의미론은 크게 어휘 의미론와 분포 의미론로 나눌 수 있다. Word2Vec은 분포 의미론의 기술 중 하나이다. 분포 의미란, 큰 텍스트 데이터 샘플에서 분포 속성에 따라 언어 항목 간의 의미 유사성을 따지고 분류하는 기술, 또는 이론 개발에 초점을 맞춘 연구 영역이다.

예를 들면, 어휘 사전에 자동차, 비행기, 자전거, 킥보드, 로켓이 있다고 가정하자. 사람이라면 자동차, 자전거, 킥보드는 땅 위를 이동하는데 쓰는 탈것이고, 비행기와 로켓은 하늘을 이동하는데 쓰는 탈것이라는 사실을 쉽게 파악할 수 있다. 하지만 머신은 특정 상황과 관련해 이들 종류의 의미를 파악해낼 수 없다. 대신, 설명한 탈것 항목은 데이터세트 속에서 특정 단어와 함께 있을 가능성이 크다는 사실을 이용해, 코퍼스의 단어 분포에 초점을 맞추는 것이다. 이렇듯 단어의 분포에 관련해, 비슷한 분포의 단어나 언어 항목이 서로 유사한 의미를 지니게 되는 것을 **분포 가설(distributed hypothesis)** 이라고 한다.

<p align="center"><img width="60%" src="https://miro.medium.com/max/2716/1*3pesTy5IjvLa2X-iX_tEKQ.png" /></p>
위 문장들에서 stars 단어는 shining, bright, dark 등과 같은 단어에 더 자주 포함된다. 이 단어들 모두 stars의 문맥과 의미를 이해하는 데 매우 유용하다. 이렇듯, 특정 단어의 의미를 어떻게 더 잘 표현할 수 있는지 뿐만 아니라 이 단어의 문맥에 나타날 다른 단어를 어떻게 예측하는지를 알려면 해당 단어의 **분포 표현(distributional representation)** 을 이해해야 한다. 단어의 분포 표현은 단어를 표현할 수 있는 벡터 형태로, 단어의 벡터 형태를 생성하려면 원-핫 인코딩이나 그 외의 기술을 사용할 수도 있지만 **유사성 측정의 중요성도 전달하는 단어의 벡터를 생성해 그 단어의 문맥상 의미를 이해할 수 있게 하는 것이 중요**하다! 이 분포 유사성 얘기할 때 Word2Vec이 필요해진다.

Word2Vec
=============
Word2Vec의 개념은 구글의 토마스 미콜로프가 이끄는 팀이 개발했다. NLP에서는 단어, 구, 문장 등의 의미를 다룰 수 있는 도구나 기술 개발이 상당히 중요하며 Word2Vec 모델은 단어, 구, 문장, 단락, 문서의 의미를 알아내는 작업을 수행한다.

정의
-------------
Word2Vec은 2계층 신경망(two-layer neural network)을 사용해 개발되어, 대용량의 텍스트 데이터(또는 텍스트 코퍼스)를 입력 받아 주어진 텍스트에서 벡터 집합을 생성한다. 2계층 신경망은 일종의 로직을 수행하며 벡터 공간의 벡터를 생성하는 블랙박스이다. 벡터 공간에서는 코퍼스 내의 각 고유 단어가 대응 벡터로 지정된다. 즉, 벡터 공간이란 큰 텍스트 코퍼스 내의 모든 단어에 대한 벡터 표현이라고 할 수 있다.

Word2Vec은 단어 유사성 발견이나 단어 간 의미 관계 보존 측면에서 뛰어난 장점을 가지고 있는데, 이런 점들은 원-핫 인코딩이나 워드넷을 사용해서는 처리할 수 없었다.

가령 원-핫 인코딩을 통해 다음과 같이 작업을 수행했다고 해보자.
<p align="center"><img width="35%" src="https://miro.medium.com/max/886/1*_da_YknoUuryRheNS-SYWQ.png" /></p>

* **원-핫 인코딩은 단어 간의 문맥 유사성에 대한 사실을 밝히지 않는다.** 만약 cat와 cats가 같은 문장에 있다고 해도 아예 다르게 벡터화가 되어 두 단어가 매우 유사한 단어라는 사실을 밝혀내지 못한다. 또, 원-핫 인코딩된 벡터에 AND 연산을 적용해도 문맥상의 유사성을 나타내지 않는다. cat와 mat의 원-핫 벡터에 AND 연산(내적, 혹은 점곱 dot product)을 적용하면 0이 된다. 실제로 두 단어는 한 문장에 함께 나타나고 강한 맥락적 관계를 가질 수 있지만, 원-핫 인코딩은 단어 유사성에 관해 중요한 것을 표현하지 않는다.

* 워드넷이 이에 충분한 도움이 되는 것도 아니다. 워드넷은 인간에 의해 구성되기 때문에 이에 포함된 것은 무엇이든 주관적이게 된다. 또, 새롭게 나타난 신조어들은 워드넷에게도 새 단어이므로 웹 사이트에 없을 수도 있어서, 이들 단어들에 대해 워드넷에서 다른 의미론적 관계를 유도해낼 수 없다. (그리고 무엇보다 많은 시간과 노력이 필요하다)

이를 해결하기 위한 방법이 Word2Vec인 것이다!

Word2Vec 모델의 컴포넌트
--------------
Word2Vec 모델에는 세 가지 주요 구성 요소가 있다.

1. Word2Vecdml 입력

원시 텍스트 코퍼스, 혹은 (실제 어플리케이션에서는) 큰 코포라를 입력으로 사용한다.

2. Word2Vec의 출력
<p align="center"><img width="35%" src="https://miro.medium.com/max/1570/0*jjpXSH7sZPfAbZWS.PNG" /></p>
일반적인 목표는, 문맥을 제공하기만 하면 그 문맥에 가장 적합한 단어의 확률을 예측하는 것이다. 위의 그림에서 sat 단어를 취해서 목표에 따라 sat 단어를 벡터 형식으로 변환하고, sat의 벡터 형태를 사용해 이 문장에서 사용된 다른 단어들인 fat, cat 등의 단어의 확률을 예측할 수 있다.

3. Word2Vec 모델의 구조 컴포넌트

신경망 기술을 사용한다. 신경망 기술은 많은 양의 데이터로 학습하는 경우에 좋은 알고리즘이 되기 때문이다.


아키텍쳐
-----------
<p align="center"><img width="70%" src="https://miro.medium.com/max/1400/1*QK2KNCZ1pczCq-uTse-dmA.png" /></p>

### 어휘 빌더 (Vocabulary Builder)
이 컴포넌트는 주로 문장의 형태로 있는 원시 텍스트 데이터를 얻는다.

파이썬의 gensim이라는 라이브러리가 있는데, 이를 사용해 코퍼스에 대해 Word2Vec을 생성한다. 자세한 매개변수는 [gensim 도큐먼테이션](https://radimrehurek.com/gensim/models/word2vec.html) 참고. 이 라이브러리를 사용한 어휘 빌더를 통해 단어 인덱스와 단어 횟수가 들어있는 어휘 객체를 출력받는다.

### 문맥 빌더 (Context Builder)
문맥 빌더는 문맥 윈도우의 일부인 단어뿐만 아니라 어휘 빌더의 출력을 입력으로 사용한다.

문맥 윈도우는 일종의 sliding window이다. 윈도우의 크기는 사용자가 지정할 수 있다(일반적으로는 5~10 단어의 문맥 윈도우 크기를 사용한다고 함).

<p align="center"><img width="35%" src="https://miro.medium.com/max/568/0*Ghg8pl2oX8r5kdks.PNG" /></p>

문맥 빌더의 출력은 단어 쌍(word pairing)이며 이들 단어 쌍은 신경망에 제공된다. 예를 들어 그림의 네번째 단계에서는 (sat, on, the)(sat, cat, fat)이 출력돼 전달되는데, 신경망은 각 단어 쌍이 나타나는 횟수로 기본 통계를 학습하기 때문에 (sat, dog, fat)보다는 (sat, cat, fat)에 대해 더 많은 훈련 예제를 얻는 것이고, 훈련이 끝났을 때 sat 단어를 입력 내용으로 넣으면 dog보다 cat에 대해 훨씬 더 높은 확률이 나온다.

### 2계층 신경망
알다싶이 신경망은 입력층, 은닉층, 출력층을 layer로 가지고 있으며 이들은 순전파와 역전파를 반복하며 가중치를 재조정한다. 이를 NLP에서는 "코퍼스로부터 다른 문맥-대상 단어 쌍을 얻어 훈련을 계속 반복한다"로 표현할 수 있을 것이다. 이를 통해 단어 간의 관계를 학습해 코퍼스로부터 단어의 벡터 표현을 개발해낸다.

Word2Vec에는 두 가지 메인 처리 알고리즘이 있다.

<p align="center"><img width="35%" src="https://www.researchgate.net/profile/Elena-Tutubalina/publication/318507923/figure/fig2/AS:613947946319904@1523388005889/Illustration-of-the-word2vec-models-a-CBOW-b-skip-gram-16-33.png" /></p>
