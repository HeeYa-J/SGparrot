Week_3 (0324-0326)
=============

**목차**
1.
2.
3.

워드 임베딩 Word Embedding
=============
저번 주에는 NLP에서 어휘 속의 단어나 구(phrase)가 실수의 벡터로 매핑되는 언어 모델과 피처 엔지니어링 기술을 살펴보았다. 워드 임베딩이란, 단어를 실수로 표현하는데 사용하는 기술이다.

원-핫 인코딩으로 만든 벡터는 희소(sparse)하고 고차원이다. 이게 무슨 뜻이냐하면, 벡터값 대부분이 0이고(원-핫 인코딩을 하면 어떤 행렬이 나오는지 떠올려보자), 어휘 사전에 있는 단어의 수와 차원이 같다. 반면, 워드 인베딩은 저차원의 실수형 벡터(밀집 벡터)이다. 

<p align="center"><img width="35%" src="https://user-images.githubusercontent.com/80621384/112938737-24cbea00-9165-11eb-8a1b-584686579515.png" /></p>

워드 임베딩 방법론으로는 LSA, Word2Vec, FastText, Glove 등이 있다. 그중 Word2Vec을 중점적으로 살펴보려고 한다.

분포 의미론 Distributed Semantics
=============
의미론은 크게 어휘 의미론와 분포 의미론로 나눌 수 있다. Word2Vec은 분포 의미론의 기술 중 하나이다. 분포 의미란, 큰 텍스트 데이터 샘플에서 분포 속성에 따라 언어 항목 간의 의미 유사성을 따지고 분류하는 기술, 또는 이론 개발에 초점을 맞춘 연구 영역이다.

예를 들면, 어휘 사전에 자동차, 비행기, 자전거, 킥보드, 로켓이 있다고 가정하자. 사람이라면 자동차, 자전거, 킥보드는 땅 위를 이동하는데 쓰는 탈것이고, 비행기와 로켓은 하늘을 이동하는데 쓰는 탈것이라는 사실을 쉽게 파악할 수 있다. 하지만 머신은 특정 상황과 관련해 이들 종류의 의미를 파악해낼 수 없다. 대신, 설명한 탈것 항목은 데이터세트 속에서 특정 단어와 함께 있을 가능성이 크다는 사실을 이용해, 코퍼스의 단어 분포에 초점을 맞추는 것이다. 이렇듯 단어의 분포에 관련해, 비슷한 분포의 단어나 언어 항목이 서로 유사한 의미를 지니게 되는 것을 분포 가설(distributed hypothesis)라고 한다.

Word2Vec
=============
Word2Vec의 개념은 구글의 토마스 미콜로프가 이끄는 팀이 개발했다. NLP에서는 단어, 구, 문장 등의 의미를 다룰 수 있는 도구나 기술 개발이 상당히 중요하며 Word2Vec 모델은 단어, 구, 문장, 단락, 문서의 의미를 알아내는 작업을 수행한다.

정의
-------------
Word2Vec은 2계층 신경망(two-layer neural network)을 사용해 개발되어, 대용량의 텍스트 데이터(또는 텍스트 코퍼스)를 입력 받아 주어진 텍스트에서 벡터 집합을 생성한다. 2계층 신경망은 일종의 로직을 수행하며 벡터 공간의 벡터를 생성하는 블랙박스이다. 벡터 공간에서는 코퍼스 내의 각 고유 단어가 대응 벡터로 지정된다. 즉, 벡터 공간이란 큰 텍스트 코퍼스 내의 모든 단어에 대한 벡터 표현이라고 할 수 있다.

Word2Vec은 단어 유사성 발견이나 단어 간 의미 관계 보존 측면에서 뛰어난 장점을 가지고 있는데, 이런 점들은 원-핫 인코딩이나 워드넷을 사용해서는 처리할 수 없었다.
