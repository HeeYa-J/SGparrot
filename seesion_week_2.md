# Week_2 (0317-0319)
**목차**
1. [텍스트 데이터](텍스트-데이터)
2. [언어 모델](언어-모델)
3. [텍스트 벡터화](텍스트-벡터화)


텍스트 데이터
=============
**텍스트(text)란 단어의 시퀀스나 문자의 시퀀스**로 이해할 수 있다.
시퀀스 처리용 딥러닝 모델은 텍스트를 사용하여 기초적인 자연어 이해(NLP) 문제를 처리할 수 있는데, 이런 모델은 문서 분류, 감성 분석, 저자 식별, 질문 응답(QA) 등의 애플리케이션에 적합하다.

그렇다면 컴퓨터는 텍스트를 어떻게 이해할까?

사실 '이해한다'라고 보기 보단 문자 언어(written language)에 대한 **통계적 구조**를 만들어 문제를 해결한다고 볼 수 있다. 컴퓨터 비전이 이미지의 픽셀에 적용한 패턴 인식인 것처럼, NLP 모델은 단어, 문장, 문단에 적용한 패턴 인식인 것이다!


언어 모델
=============
언어 모델(Languagel Model)이란 단어의 순서(시퀀스, 혹은 문장)에 대한 확률 분포이다. 위에서 말했던 '통계적 구조'를 만들어 확률값을 할당한다고도 할 수 있다. 
문장의 구조가 완벽할 수록, 즉 "아, 이건 말이 된다"고 판단될 수록 더 높은 확률을 부여한다.

언어 모델은 크게 통계적 언어모델(Statistical Language Model)과 N-gram 언어모델(N-gram Language Model)로 나눌 수 있다.

통계적 언어모델(SLM) 
-------------
조건부 확률을 이용하여, 현재까지의 문자 시퀀스 바로 뒤에 나올 가장 높은 확률의 단어를 찾는 모델이다. 
특정 영역에서의 말뭉치를 기반으로 단어 set을 만들어두고 그 안에서 확률을 계산해 단어를 찾는 방식이기 때문에 context-dependent한 성격을 가진다.
특정 domain에 대한 dependency가 크다는 뜻인데, 통계적 언어모델은 학습 데이터에 매우 민감하다.

초창기에 나온 것이 **유니그램 모델(Unigram Model)**이다.

각 단어가 서로 독립이라고 가정한다면, n개의 단어가 동시에 나타날 확률은 다음과 같다.

![CodeCogsEqn](https://user-images.githubusercontent.com/80621384/112172648-3cbcde80-8c38-11eb-93d0-f72560d62630.gif)

유니그램 모델은 다음과 같은 테이블로 구성된다. 학습말뭉치에 등장한 각 단어 빈도를 세어서 전체 단어수로 나누어준 것이다.
<p align="center"><img width="20%" src="https://user-images.githubusercontent.com/80621384/112174134-822ddb80-8c39-11eb-864b-bd9786a83b07.jpg" /></p>

텍스트마이닝 학습말뭉치로 학습한, 위와 같은 유니그램 모델 θ가 주어진 상황에서 ‘world’와 ‘we’, ‘share’이라는 세 개 단어로 구성된 첫번째 문서 D의 출현확률을 구해보자.
<p align="center"><img width="38%" src="https://user-images.githubusercontent.com/80621384/112175521-a6d68300-8c3a-11eb-9af2-5f2f61c7a107.gif" /></p>


### 희소 문제(Sparsity Problem)
통계적 언어모델의 단점은 학습말뭉치에 존재하지 않는 단어의 경우 그 확률이 0이 된다는 것이다. 
이미 언급했던 것처럼 학습말뭉치에 의존적이기 때문에 범용적인 모델을 구축하기가 어렵다. 
따라서 information retrieval context에서는 확률이 0이 되는 경우를 피하기 위해 종종 단일그램 언어 모델을 smoothen한다. 
공통 접근법은 전체 수집에 대한 최대 우도 모델을 생성하고 각 문서에 대한 최대 우도 모델로 선형 보간하여 모델을 smoothen하는 것이다.


N-gram 모델
-------------
위의 Sparsity Problem을 보완하기 위해 고안된 방법이다. 
주어진 문장 전체가 아닌 바로 양 옆의 N개의 단어도 보고 다음 단어를 예측하는 방법인데, N-gram이란 '텍스트를 나누는 단위'라는 뜻이기 때문에 N의 크기에 따라 종류가 나뉜다.

예를 들면, 의미가 완전히 반대인 두 문자열 "it's bad, not good at all"과 "it's good, not bad at all"이 완전히 동일하게 변환된다. 단어 앞에 등장하는 "not"은 문맥의 중요성을 잘
보여주는 요소라 볼 수 있다. 이때 문맥도 반드시 고려해야 의미를 유지한 채로 변환할 수 있는데, **토큰 하나의 횟수만 고려하지 않고 옆에 있는 두세 개의 토큰을 함께 고려하는 것**이다.
아래와 같은 종류들이 있으며 일반적으로 연속된 토큰을 n-gram이라고 한다.

* unigrams : n이 1인 경우
* bigrams : n이 2인 경우
* trigrams : n이 3인 경우

<p align="center"><img width="35%" src="https://user-images.githubusercontent.com/80621384/112178669-4b59c480-8c3d-11eb-936c-e9577609fa61.png" /></p>


텍스트 벡터화
=============
기본적으로 딥러닝 모델은 수치형 텐서만 다룰 수 있다. 따라서 텍스트를 수치형 텐서로 변환하는 과정이 필요한데, 이 과정이 텍스트 벡터화(vectorizing text)이다.
텍스트를 나누는 단위(단어, 문자, n-그램)를 토큰(token)이라고 하고, 텍스트를 토큰으로 나누는 작업을 토큰화(tokenization)라고 한다.

모든 텐스트 벡터화 과정은 어떤 종류의 토큰화를 적용하고 생성된 토큰에 수치형 벡터를 연결하는 것으로 이루어진다. 이렇게 해서 나온 벡터는 시퀀서 텐서로 묶여 심층 신경망에 주입된다.



BoW
-------------
머신러닝에서 텍스트를 표현하는 방법 중 BoW(Bag of Words)는 가장 간단하지만 효과적인 방법이다. 이 방법을 쓰면 장, 문단, 문장, 서식 같은 입력 텍스트의 구조 대부분을 잃고, 각 단어가 이 말뭉치에
있는 **텍스트에 얼마나 많이 나타나는지만** 헤아린다. 구조와 상관없이 단어의 출현 횟수만 세기 때문에 'Bag'라는 표현을 쓴다고 생각하면 쉽다.

BOW 표현을 계산하기 위해서는 다음 세 단계를 거친다.
1. 토큰화 - 공백이나 구두점 등을 기준으로 분리할 수 있음
2. 어휘 사전 구축 - 모든 문서에 나타난 모든 단어의 어휘를 모으고 번호를 매김(알파벳 순서)
3. 인코딩 - 어휘 사전의 단어가 문서마다 몇 번 나타나는지 헤아림

<p align="center"><img width="65%" src="https://user-images.githubusercontent.com/80621384/112180755-2d8d5f00-8c3f-11eb-9e49-363e3e58612f.jpg" /></p>

출력은 각 문서에서 나타난 단어의 횟수가 담긴 하나의 벡터이다. 즉, BoW는 각 단어가 등장한 횟수를 수치화하기 때문에 이 수치 표현은 전체 데이터셋에서 고유한 각 단어를 특성으로 가진다.
원본 문자열에 있는 단어의 순서는 BOW 특성 표현에서 완전히 무시된다.


DTM
-------------
DTM(Document-Term Matrix)은 위 BoW의 표현 방법대로 표현하되 **복수의 문서를 하나의 행렬에서 표현한 것**이다.

가령 맨 왼쪽의 인덱스를 문서0, 문서1,... 이라고 했을 때, DTM을 통해 각 문서의 단어들을 하나의 행렬로 묶어서 표현할 수 있다.

<p align="center"><img width="35%" src="https://s3.ap-northeast-2.amazonaws.com/breezy-blog-seoul/2018/02/count-dtm.png" /></p>

위의 예시에서는 한 DTM을 단어 7개의 Bow로 표현했으니까 상대적으로 적은 영역을 사용했다. 하지만 각 칼럼이 문서들에 등장하는 모든 단어를 포함한다고 생각해보자.
한 문서가 가지고 있는 단어를 다른 문서가 가지고 있으리란 보장은 없기 때문에 대부분의 공간을 0이 차지하게 된다. 이는 곳 공간적, 시간적 리소스의 낭비이다.
특히 대용량의 문서일 경우 엄청난 리소스 낭비가 일어날 것임을 예상할 수 있다.

또 하나의 단점은 빈도 수로 접근하는 것의 위험성인데, 'a'나 'the'가 많이 등장한다면 해당 문서들을 '유사하다'고 판단해도 될까?
모든 문서에는 너무 빈번하여 유용하지 않은 단어가 존재하는데, 이런 의미 없는 불용어(stopword)를 잘 제거해주어야 모델 성능 향상에 도움이 된다.

TF-IDF
-------------
TF-IDF(term frequency-inverse document frequency, 단어빈도-역문서빈도)는 중요하지 않아 보이는 특성을 제외하는 대신, 얼마나 의미 있는 특성인지를 계산해서 스케일을 조정하는 방법이다.
TF-IDF는 말뭉치의 다른 문서보다 **특정 문서에 자주 나타나는 단어**에 높은 가중치를 준다. 
한 단어가 특정 문서에 자주 나타나고 다른 여러 문서에는 그렇지 않다면, 오히려 그 문서의 내용을 아주 잘 설명하는 단어라고 보는 것이다!

TF-IDF 스케일 변환 방식은 여러 변종이 있으니 위키백과를 참고하자 - <https://ko.wikipedia.org/wiki/Tf-idf>

파이썬 모듈 scikit-learn은 TfidTransformer과 TfidVectorizer 클래스에 tf-idf를 구현해놓았다. 
이 클래스들에 따르면, 문서 d에 있는 단어 w에 대한 TF-IDF 점수는 다음과 같이 정의되어 있다.
<p align="center"><img width="30%" src="https://user-images.githubusercontent.com/80621384/112184854-f7ea7500-8c42-11eb-9b79-711f6f70e96c.gif" /></p>

* N: 훈련 세트에 있는 문서의 개수
* Nw: 단어 w가 나타난 훈련 세트 문서의 개수
* tf(단어 빈도수): 단어 w가 대상 문서 d(변환 또는 인코딩하려는 문서)에 나타난 횟수

두 파이썬 클래스 모두 tf-idf 계산을 한 후에 L2 정규화(L2 normalization)을 적용한다.
이렇게 scaling 된 벡터는 문서의 길이(단어의 수)에 영향을 받지 않는다.


잠재 의미 분석(LSA)
-------------
BoW에 기반한 DTM이나 TF-IDF는 기본적으로 단어의 빈도 수를 이용한 수치화 방법이기 때문에 단어의 의미를 고려하지 못한다.
이를 위한 대안으로 DTM의 **잠재된(Latent) 의미를 이끌어내는 방법**으로 잠재 의미 분석(Latent Semantic Analysis, LSA)이라는 방법이 탄생했는데, 
선형대수학의 특이값 분해(Singular Value Decomposition, SVD)를 기반으로 설명된다.

SVD란 A가 m × n 행렬일 때, 다음과 같이 3개의 행렬의 곱으로 분해(decomposition)하는 것을 말한다.
<p align="left"><img width="10%" src="https://user-images.githubusercontent.com/80621384/112186573-9f1bdc00-8c44-11eb-892d-989a099bb028.png" /></p>
여기서 각 3개의 행렬은 다음과 같은 조건을 만족한다.
<p align="left"><img width="35%" src="https://user-images.githubusercontent.com/80621384/112186576-9fb47280-8c44-11eb-8c0c-2ba39adaf3b1.png" /></p>

위에서 설명한 SVD를 full SVD라고 하는데, LSA의 경우 full SVD에서 나온 3개의 행렬에서 일부 벡터들을 삭제시킨 절단된 truncated SVD를 사용하게 된다.

<p align="center"><img width="65%" src="https://miro.medium.com/max/1700/0*keD6cBBsvWG6gCbu.jpg" /></p>
이렇게 일부 벡터들을 삭제하는 것을 "데이터의 차원을 축소시킨다"라고 하는데, 데이터의 차원을 축소하면 full SVD를 하였을 때보다 
1. 계산 비용이 낮아지는 효과가 있고
2. 상대적으로 중요하지 않은 정보를 삭제함으로써 설명력이 높은 정보를 남긴다!

잠재 디레클레 할당(LDA)
-------------
직관적으로 생각하면 잠재 디레클레 할당(Latent Dirichlet Allocation, LDA) 모델은 **함께 자주 나타나는 단어의 그룹(토픽)을 찾는 것**이다. LDA는 각 문서에 토픽의 일부가 혼합되어 있다고 간주하는데,
머신러닝에서의 토픽은 우리가 일상 대화에서 말하는 '주제'가 아니고, 의미가 있든 없든 PCA나 NMF로 추출한 성분에 가까운 것이다.

뉴스 기사를 예로 들자면, 두 명의 기자가 쓴 스포츠, 정치, 금융에 관한 기사가 있다고 가정해보자.
정치 기사에서는 "선거", "시장", "정부" 등의 단어가 많이 나올 것이라고 예상할 수 있고, 스포츠 기사에서는 "선수", "진출", "팀" 같은 단어가 자주 등장할 것이다.
각 그룹별 단어들은 함께 나타나는 경우가 많으며, 반대로 "정부"와 "선수"라는 단어는 함께 나타나는 경우가 드물다.
그렇다고 해서 각 그룹은 꼭 동시에 나타날 것으로 예상되는 단어들로만 구성되어 있지는 않다.
기자1은 "한편"이라는 단어를 즐겨 쓰고, 기자2는 "다시 말해서"라는 단어를 즐겨 쓸 수도 있다.
이때 토픽은 기자1이 즐겨 쓰는 단어와 기자2가 즐겨 쓰는 단어가 될 수 있다. 그렇다 하면 이 토픽은 일반적인 주제와는 관련이 없어진다.

즉, LDA를 수행할 때 문서 집합에서 토픽이 몇 개가 존재할지 가정하는 것은 사용자가 해야 할 일이다.
sk-learn 모듈로부터 LatentDirichletAllocation을 import 해 올 수 있고, 파라미터 중 n_components에 정하고 싶은 토픽의 수를 입력해 모델링 할 수 있다.
LDA는 앞서 배운 빈도수 기반의 표현 방법인 BoW의 행렬 DTM 또는 TF-IDF 행렬을 입력으로 하는데, LDA는 단어의 순서는 신경쓰지 않는다는 사실을 알 수 있다.

<p align="center"><img width="75%" src="https://www.researchgate.net/profile/Diego-Buenano-Fernandez/publication/339368709/figure/fig1/AS:860489982689280@1582168207260/Schematic-of-LDA-algorithm.png" /></p>

